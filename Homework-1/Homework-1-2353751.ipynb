{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55bacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem A\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "data = np.loadtxt('Homework-1-Data.txt', skiprows=1)\n",
    "labels = np.concatenate([np.ones(36499), np.zeros(93565)])\n",
    "Accuracy_comparison = \"\"\n",
    "auc_comparison = \"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=labels  # Keep class proportions\n",
    ")\n",
    "\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    learning_rate=0.1,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "def results_display(model_name: str):\n",
    "    global Accuracy_comparison, auc_comparison\n",
    "\n",
    "    print(model_name+f\" Accuracy: {accuracy:.4f}\")\n",
    "    print(model_name+f\" AUC: {auc_score:.4f}\")\n",
    "    Accuracy_comparison += model_name+f\" Accuracy: {accuracy:.4f}\\n\"\n",
    "    auc_comparison += model_name+f\" AUC: {auc_score:.4f}\\n\"\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.4f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(model_name + ' ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    if(model_name == \"BDT\"):\n",
    "        return\n",
    "    \n",
    "    plt.plot(model_history.history['loss'], label='Training Loss')\n",
    "    plt.plot(model_history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(model_name + ' Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "results_display(\"BDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c956a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem B\n",
    "feature_importance = model.feature_importances_\n",
    "top_10_indices = np.argsort(feature_importance)[-10:][::-1]  # Top 10 most important features\n",
    "\n",
    "print(\"Top 10 F-score:\")\n",
    "for i, idx in enumerate(top_10_indices):\n",
    "    print(f\"{i+1:2d}. Feature {idx:2d}: {feature_importance[idx]:.6f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(10), feature_importance[top_10_indices])\n",
    "plt.xlabel('Top 10 Features')\n",
    "plt.ylabel('F-score (Feature Importance)')\n",
    "plt.title('Top 10 Most Important Features (F-score)')\n",
    "plt.xticks(range(10), [f'Feature {idx}' for idx in top_10_indices], rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find the most important feature\n",
    "most_important_idx = top_10_indices[0]\n",
    "most_important_score = feature_importance[most_important_idx]\n",
    "\n",
    "print(f\"\\nMost Important Feature: Feature {most_important_idx}\")\n",
    "print(f\"F-score: {most_important_score:.6f}\")\n",
    "\n",
    "# Plot histogram of the most important feature\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Get the most important feature values for test set\n",
    "feature_values = X_test[:, most_important_idx]\n",
    "signal_values = feature_values[y_test == 1]  # Signal (label = 1)\n",
    "background_values = feature_values[y_test == 0]  # Background (label = 0)\n",
    "\n",
    "# Calculate bin edges\n",
    "min_val = np.min(feature_values)\n",
    "max_val = np.max(feature_values)\n",
    "bins = np.linspace(min_val, max_val, 101)  # 100 bins\n",
    "\n",
    "# Plot histograms\n",
    "plt.hist(signal_values, bins=bins, alpha=0.7, label='Signal', color='red', density=True)\n",
    "plt.hist(background_values, bins=bins, alpha=0.7, label='Background', color='blue', density=True)\n",
    "\n",
    "plt.xlabel(f'Feature {most_important_idx} Value')\n",
    "plt.ylabel('Density')\n",
    "plt.title(f'Distribution of Most Important Feature {most_important_idx}\\n(Signal vs Background)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical analysis of the most important feature\n",
    "print(f\"\\nFeature {most_important_idx} Statistics:\")\n",
    "print(f\"Signal (label=1):\")\n",
    "print(f\"  Mean: {np.mean(signal_values):.4f}\")\n",
    "\n",
    "print(f\"\\nBackground (label=0):\")\n",
    "print(f\"  Mean: {np.mean(background_values):.4f}\")\n",
    "\n",
    "print(\"\\n Signal tends to have HIGHER values than background\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3456c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem C\n",
    "from urllib.parse import _ResultMixinStr\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='tanh', input_shape=(50,)),\n",
    "    Dense(128, activation='tanh'),\n",
    "    Dense(128, activation='tanh'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.01),\n",
    "    loss=BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "y_pred_proba = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "results_display(\"NN+tanh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddedea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem D\n",
    "print(\"As seen below, without standardization, using the ReLU activation function makes it completely impossible to train.\")\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(50,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.01),\n",
    "    loss=BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "y_pred_proba = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "results_display(\"NN+ReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34129914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem E\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(50,)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),  # Adam optimizer with default learning rate\n",
    "    loss=BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model_history = model.fit(\n",
    "    X_train, y_train,  # Use scaled training data\n",
    "    batch_size=128,\n",
    "    epochs=50,\n",
    "    validation_data=(X_test, y_test),  # Use scaled test data\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "y_pred_proba = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "results_display(\"NN+ReLU+Scaler+Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6544041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#problem F\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(200, activation='relu', input_shape=(50,), kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(200, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(200, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(200, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(200, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(200, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(200, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(200, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(200, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(200, activation='relu', kernel_regularizer=l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    loss=BinaryCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model_history = model.fit(\n",
    "    X_train, y_train,  \n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_proba = model.predict(X_test).flatten()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc_score = auc(fpr, tpr)\n",
    "\n",
    "results_display(\"Deep_NN+ReLU+L2+Adam+LR_Decay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "429e3bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BDT Accuracy: 0.9446\n",
      "NN+tanh Accuracy: 0.9446\n",
      "NN+ReLU+Scaler+Adam Accuracy: 0.7194\n",
      "Deep_NN+ReLU+L2+Adam+LR_Decay Accuracy: 0.9360\n",
      "\n",
      "\n",
      "BDT AUC: 0.9850\n",
      "NN+tanh AUC: 0.8882\n",
      "NN+ReLU+Scaler+Adam AUC: 0.9776\n",
      "Deep_NN+ReLU+L2+Adam+LR_Decay AUC: 0.9802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#summary\n",
    "print(Accuracy_comparison+\"\\n\")\n",
    "print(auc_comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4sci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
